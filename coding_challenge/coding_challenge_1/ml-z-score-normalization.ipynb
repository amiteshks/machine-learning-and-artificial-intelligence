{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Z-Score Normalization\n",
    "\n",
    "Write a function to standardize a dataset using **Z-score normalization**.\n",
    "\n",
    "A Z-score is calculated as:\n",
    "\n",
    "(data â€“ mean)/(standard deviation)\n",
    "\n",
    "\n",
    "**Hint:** Use NumPy to calculate the mean and standard deviation.\n",
    "\n",
    "**Example:**\n",
    "Input: `[1.0, 2.0, 3.0, 4.0, 5.0]`  \n",
    "Output: `[-1.41, -0.71, 0.0, 0.71, 1.41]` *(approx)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ZScoreNormalization:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_normalized_z_scores(self, input_data):\n",
    "        std = np.std(input_data)\n",
    "        \n",
    "        if std == 0:\n",
    "            raise ValueError(\"Standard deviation is zero, cannot normalize.\")\n",
    "\n",
    "        z_data = np.divide(np.subtract(input_data, np.mean(input_data)), std)\n",
    "       \n",
    "        return z_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "We will validate the function using two approaches:\n",
    "\n",
    "1. **Direct Method Invocation**  \n",
    "   Calling the method manually to verify the output.\n",
    "\n",
    "2. **Automated Testing with `ipytest`**  \n",
    "   Writing test cases using the `ipytest` framework to ensure correctness and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n"
     ]
    }
   ],
   "source": [
    "# 1. **Direct Method Invocation** - Calling the method manually to verify the output.\n",
    "\n",
    "z_score_normalization = ZScoreNormalization()\n",
    "\n",
    "print(z_score_normalization.get_normalized_z_scores([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. **Automated Testing with `ipytest`** - Writing test cases using the `ipytest` framework to ensure correctness and reliability.\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "def test_get_normalized_z_scores():\n",
    "    \n",
    "    expected = [-1.41421356, -0.70710678, 0., 0.70710678, 1.41421356]\n",
    "    actual = z_score_normalization.get_normalized_z_scores([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "    assert np.allclose(actual, expected, atol=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
