{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e726c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, middleware\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph import StateGraph\n",
    "import openai\n",
    "import logging\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langchain_openai import ChatOpenAI  #   LangChain: OpenAI Chat Model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage  #   LangChain: Message Handling\n",
    "import os, getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from healthcare_tools import discharge_patient, get_patient_status\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from healthcare_tools import discharge_patient, get_patient_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a3ebc7-6cb8-44a7-8533-c7c1b9283060",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticAI-POC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf9fff6-011d-4792-9973-11275733ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discharge_patient_tool(patient_id: str) -> str:\n",
    "    \"\"\"Discharge a patient from the hospital given their patient ID.\n",
    "\n",
    "    Args:\n",
    "        patient_id: the patient ID of the patient to discharge\n",
    "    \"\"\"\n",
    "    discharge_patient(patient_id)\n",
    "    \n",
    "    return f\"Patient {patient_id} discharged successfully.\" \n",
    "\n",
    "def get_patient_status_tool(patient_id: str) -> str:\n",
    "    \"\"\"Get the status of a patient given their patient ID.\n",
    "\n",
    "    Args:\n",
    "        patient_id: the patient ID of the patient to get the status of\n",
    "    \"\"\"\n",
    "    status = get_patient_status(patient_id)\n",
    "    \n",
    "    return f\"Patient {patient_id} status is {status}.\" \n",
    "\n",
    "\n",
    "tools = [discharge_patient_tool, get_patient_status_tool]\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math\n",
    "# the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/\n",
    "# play around with it and see how the model behaves with math equations!\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad9110-2c2d-426b-b394-53479902f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant. Either answer a question or invoke the following tools:\"\\\n",
    "          \"discharge_patient(patient_id)\"\\\n",
    "          \"get_patient_status(patient_id)\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   # return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "    # Invoke the LLM with tools\n",
    "    response = llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
    "    \n",
    "    # Ensure assistant properly formats response\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Allow requests from frontend (localhost:3000)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # Change this in production!\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allow all HTTP methods\n",
    "    allow_headers=[\"*\"],  # Allow all headers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4891d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define user input model\n",
    "class UserRequest(BaseModel):\n",
    "    user_input: str\n",
    "\n",
    "# Define agent behavior (Example: LLM calling function)\n",
    "def llm_agent(input_text):\n",
    "  \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an AI assistant.. \"),\n",
    "        HumanMessage(content=input_text),\n",
    "    ]\n",
    "    response = llm.invoke(messages) \n",
    "    return response.content\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/process\")\n",
    "async def process_message(user_input: str):\n",
    "    # your code here\n",
    "    return {\"response\": \"some response\"}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class State(str):\n",
    "    str\n",
    "    \n",
    "def custom_tools_condition(state: MessagesState):\n",
    "    \"\"\"Custom condition to determine if we should route to tools or end.\n",
    "    \n",
    "    Args:\n",
    "        state: The current state containing messages\n",
    "        \n",
    "    Returns:\n",
    "        str: Either \"tools\" to route to tools node or \"end\" to finish\n",
    "    \"\"\"\n",
    "    # Get the last message from the assistant\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check if the message has tool calls\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Add any custom routing logic here\n",
    "    # For example, you could check message content for specific keywords\n",
    "    # if \"schedule surgery\" in last_message.content.lower():\n",
    "    #     return \"tools\"\n",
    "    \n",
    "    # If no tool calls needed, end the chain\n",
    "    return \"end\"\n",
    "\n",
    "# Graph\n",
    "workflow_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "workflow_builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "workflow_builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "workflow_builder.add_edge(START, \"assistant\")\n",
    "workflow_builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    custom_tools_condition,\n",
    "     {\n",
    "        \"tools\": \"tools\",  # If condition returns \"tools\", go to tools node\n",
    "        \"end\": END,        # If condition returns \"end\", go to END state\n",
    "    }\n",
    ")\n",
    "workflow_builder.add_edge(\"tools\", \"assistant\")  \n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "workflow = workflow_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(workflow.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# messages = [HumanMessage(content=\"What does PBC stand for?\")]\n",
    "messages = [HumanMessage(content=\"Start discharge process for patient 10001\")]\n",
    "messages = workflow.invoke({\"messages\": messages}, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df58b40-1be4-4ac6-9196-9cd6201c6ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
